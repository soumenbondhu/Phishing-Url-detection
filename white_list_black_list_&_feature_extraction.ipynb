{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phishing Website Detection - \n",
    "### Black and White list  list cheaking  and Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>legititimate urls</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://www.liquidgeneration.com/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://www.onlineanime.org/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://www.ceres.dti.ne.jp/~nekoi/senno/senfir...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://www.galeon.com/kmh/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://www.fanworkrecs.com/</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   legititimate urls\n",
       "0                   http://www.liquidgeneration.com/\n",
       "1                        http://www.onlineanime.org/\n",
       "2  http://www.ceres.dti.ne.jp/~nekoi/senno/senfir...\n",
       "3                         http://www.galeon.com/kmh/\n",
       "4                        http://www.fanworkrecs.com/"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "white_list=pd.read_csv(\"legitimate_urls.txt\",header=None,names=['legititimate urls'])\n",
    "white_list.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>phishing urls</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://www.liquidgeneration.com/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://www.onlineanime.org/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://www.ceres.dti.ne.jp/~nekoi/senno/senfir...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://www.galeon.com/kmh/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://www.fanworkrecs.com/</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       phishing urls\n",
       "0                   http://www.liquidgeneration.com/\n",
       "1                        http://www.onlineanime.org/\n",
       "2  http://www.ceres.dti.ne.jp/~nekoi/senno/senfir...\n",
       "3                         http://www.galeon.com/kmh/\n",
       "4                        http://www.fanworkrecs.com/"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "black_list=pd.read_csv(\"legitimate_urls.txt\",header=None,names=['phishing urls'])\n",
    "black_list.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enter a New url for Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_url=input(\"Enter your Url\")\n",
    "#\n",
    "#then press enter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(new_url)\n",
    "new_url=\"http://www.fanworkrecs.com/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cheaking in Black list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "flag=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef black_list_cheaking(black_list,new_url):\\n    for i in range(0,len(black_list)):\\n        #print(black_list[i])\\n        if (new_url==black_list[\\'phishing urls\\'][i]):\\n            print(\"the new_url\",new_url )\\n            print(\"is phishing webite\")\\n            break\\n    return\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def black_list_cheaking(black_list,new_url):\n",
    "    for i in range(0,len(black_list)):\n",
    "        #print(black_list[i])\n",
    "        if (new_url==black_list['phishing urls'][i]):\n",
    "            print(\"the new_url\",new_url )\n",
    "            print(\"is phishing webite\")\n",
    "            break\n",
    "    return\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def black_list_cheaking(black_list,new_url):\n",
    "    for i in range(0,len(black_list)):\n",
    "        #print(black_list[i])\n",
    "        if (new_url==black_list['phishing urls'][i]):\n",
    "            return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the new_url http://www.fanworkrecs.com/\n",
      "is phishing webite\n"
     ]
    }
   ],
   "source": [
    "flag=black_list_cheaking(black_list,new_url)\n",
    "if(flag==-1):\n",
    "    print(\"the new_url\",new_url )\n",
    "    print(\"is phishing webite\")\n",
    "else:\n",
    "    print(\"the new_url\",new_url )\n",
    "    print(\"is not phihing black list\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cheaking in White list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef white_list_cheaking(white_list,new_url):\\n    for i in range(0,len(white_list)):\\n        #print(black_list[i])\\n        if (new_url==white_list[\\'legititimate urls\\'][i]):\\n            print(\"the new_url\",new_url )\\n            print(\"is legitimate webite\")\\n            break\\n    print(\"the new_url\",new_url )\\n    print(\"is not legitimate white list\")\\n    return\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def white_list_cheaking(white_list,new_url):\n",
    "    for i in range(0,len(white_list)):\n",
    "        #print(black_list[i])\n",
    "        if (new_url==white_list['legititimate urls'][i]):\n",
    "            print(\"the new_url\",new_url )\n",
    "            print(\"is legitimate webite\")\n",
    "            break\n",
    "    print(\"the new_url\",new_url )\n",
    "    print(\"is not legitimate white list\")\n",
    "    return\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def white_list_cheaking(white_list,new_url):\n",
    "    for i in range(0,len(white_list)):\n",
    "        #print(black_list[i])\n",
    "        if (new_url==white_list['legititimate urls'][i]):\n",
    "            return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "th1 new_url http://www.fanworkrecs.com/\n",
      "is legitimate website\n"
     ]
    }
   ],
   "source": [
    "flag=white_list_cheaking(white_list,new_url)\n",
    "if(flag==1):\n",
    "    print(\"th1 new_url\",new_url )\n",
    "    print(\"is legitimate website\")\n",
    "else:\n",
    "    print(\"the new_url\",new_url )\n",
    "    print(\"is not legitimate white list\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# packages\n",
    "import pandas as pd\n",
    "from urllib.parse import urlparse,urlencode\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "#import whois\n",
    "import whois\n",
    "import urllib.request\n",
    "from datetime import datetime\n",
    "import time\n",
    "import socket\n",
    "from urllib.error import HTTPError\n",
    "from cython.parallel import prange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "itime = datetime.now()\n",
    "for i in prange(0,10000):\n",
    "    pass\n",
    "ftime = datetime.now()\n",
    "#print(ftime-itime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd = None\n",
    "class FeatureExtraction:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def getProtocol(self,url):\n",
    "        return urlparse(url).scheme\n",
    "    \n",
    "    def getDomain(self,url):\n",
    "        return urlparse(url).netloc\n",
    "    \n",
    "    def getPath(self,url):\n",
    "        return urlparse(url).path\n",
    "    \n",
    "    def havingIP(self,url):\n",
    "        \"\"\"If the domain part has IP then it is phishing otherwise legitimate\"\"\"\n",
    "        match=re.search('(([01]?\\\\d\\\\d?|2[0-4]\\\\d|25[0-5])\\\\.([01]?\\\\d\\\\d?|2[0-4]\\\\d|25[0-5])\\\\.([01]?\\\\d\\\\d?|2[0-4]\\\\d|25[0-5])\\\\.([01]?\\\\d\\\\d?|2[0-4]\\\\d|25[0-5])\\\\/)|'  #IPv4\n",
    "                    '((0x[0-9a-fA-F]{1,2})\\\\.(0x[0-9a-fA-F]{1,2})\\\\.(0x[0-9a-fA-F]{1,2})\\\\.(0x[0-9a-fA-F]{1,2})\\\\/)'  #IPv4 in hexadecimal\n",
    "                    '(?:[a-fA-F0-9]{1,4}:){7}[a-fA-F0-9]{1,4}',url)     #Ipv6\n",
    "        if match:\n",
    "            #print match.group()\n",
    "            return 1            # phishing\n",
    "        else:\n",
    "            #print 'No matching pattern found'\n",
    "            return 0            # legitimate\n",
    "    \n",
    "    def long_url(self,url):\n",
    "        \"\"\"This function is defined in order to differntiate website based on the length of the URL\"\"\"\n",
    "        if len(url) < 54:\n",
    "            return 0            # legitimate\n",
    "        elif len(url) >= 54 and len(url) <= 75:\n",
    "            return 2            # suspicious\n",
    "        else:\n",
    "            return 1            # phishing\n",
    "    \n",
    "    def have_at_symbol(self,url):\n",
    "        \"\"\"This function is used to check whether the URL contains @ symbol or not\"\"\"\n",
    "        if \"@\" in url:\n",
    "            return 1            # phishing\n",
    "        else:\n",
    "            return 0            # legitimate\n",
    "    \n",
    "    def redirection(self,url):\n",
    "        \"\"\"If the url has symbol(//) after protocol then such URL is to be classified as phishing \"\"\"\n",
    "        if \"//\" in urlparse(url).path:\n",
    "            return 1            # phishing\n",
    "        else:\n",
    "            return 0            # legitimate\n",
    "        \n",
    "    def prefix_suffix_separation(self,url):\n",
    "        \"\"\"If the domain has '-' symbol then it is considered as phishing site\"\"\"\n",
    "        if \"-\" in urlparse(url).netloc:\n",
    "            return 1            # phishing\n",
    "        else:\n",
    "            return 0            # legitimate\n",
    "        \n",
    "    def sub_domains(self,url):\n",
    "        \"\"\"If the url has more than 3 dots then it is a phishing\"\"\"\n",
    "        if url.count(\".\") < 3:\n",
    "            return 0            # legitimate\n",
    "        elif url.count(\".\") == 3:\n",
    "            return 2            # suspicious\n",
    "        else:\n",
    "            return 1            # phishing\n",
    "        \n",
    "    def shortening_service(self,url):\n",
    "        \"\"\"Tiny URL -> phishing otherwise legitimate\"\"\"\n",
    "        match=re.search('bit\\.ly|goo\\.gl|shorte\\.st|go2l\\.ink|x\\.co|ow\\.ly|t\\.co|tinyurl|tr\\.im|is\\.gd|cli\\.gs|'\n",
    "                    'yfrog\\.com|migre\\.me|ff\\.im|tiny\\.cc|url4\\.eu|twit\\.ac|su\\.pr|twurl\\.nl|snipurl\\.com|'\n",
    "                    'short\\.to|BudURL\\.com|ping\\.fm|post\\.ly|Just\\.as|bkite\\.com|snipr\\.com|fic\\.kr|loopt\\.us|'\n",
    "                    'doiop\\.com|short\\.ie|kl\\.am|wp\\.me|rubyurl\\.com|om\\.ly|to\\.ly|bit\\.do|t\\.co|lnkd\\.in|'\n",
    "                    'db\\.tt|qr\\.ae|adf\\.ly|goo\\.gl|bitly\\.com|cur\\.lv|tinyurl\\.com|ow\\.ly|bit\\.ly|ity\\.im|'\n",
    "                    'q\\.gs|is\\.gd|po\\.st|bc\\.vc|twitthis\\.com|u\\.to|j\\.mp|buzurl\\.com|cutt\\.us|u\\.bb|yourls\\.org|'\n",
    "                    'x\\.co|prettylinkpro\\.com|scrnch\\.me|filoops\\.info|vzturl\\.com|qr\\.net|1url\\.com|tweez\\.me|v\\.gd|tr\\.im|link\\.zip\\.net',url)\n",
    "        if match:\n",
    "            return 1               # phishing\n",
    "        else:\n",
    "            return 0               # legitimate\n",
    "        \n",
    "    \"\"\"\n",
    "    def google_index(self,url):\n",
    "        user_agent = 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/48.0.2564.116 Safari/537.36'\n",
    "        headers = { 'User-Agent' : user_agent}\n",
    "        query = {'q': 'info:' + url}\n",
    "        google = \"https://www.google.com/search?\" + urlencode(query)\n",
    "        #data = requests.get(google, headers=headers,proxies=proxies)\n",
    "        data = requests.get(google,headers=headers)\n",
    "        data.encoding = 'ISO-8859-1'\n",
    "        soup = BeautifulSoup(str(data.content), \"html.parser\")\n",
    "        try:\n",
    "            check = soup.find(id=\"rso\").find(\"div\").find(\"div\").find(\"h3\").find(\"a\")\n",
    "            if soup.find(id=\"rso\").find(\"div\").find(\"div\").find(\"h3\").find(\"a\").find(\"href\" != None):\n",
    "                href = check['href']\n",
    "                return 0 # indexed\n",
    "            else:\n",
    "                return 1\n",
    "        except AttributeError:\n",
    "            return 1 # indexed\n",
    "        #print(\"Waiting \" + str(seconds) + \" seconds until checking next URL.\\n\")\n",
    "        #time.sleep(float(seconds))\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def abnormal_url(self,url):\n",
    "        dns = 0\n",
    "        #domain_name = \"\"\n",
    "        try:\n",
    "            #domain = urlparse(url).netloc\n",
    "            #print(domain)\n",
    "            domain_name = whois.whois(urlparse(url).netloc)\n",
    "            #print(domain_name)\n",
    "        except:\n",
    "            dns = 1\n",
    "        \n",
    "        if dns == 1:\n",
    "            return 1 # phishing\n",
    "        else:\n",
    "            hostname=domain_name.domain_name\n",
    "            #match=re.search(hostname,url)\n",
    "            if hostname in url:\n",
    "                return 0 # legitimate\n",
    "            else:\n",
    "                return 1 # phishing\n",
    "    \"\"\"\n",
    "    \n",
    "    def web_traffic(self,url):\n",
    "        try:\n",
    "            rank = BeautifulSoup(urllib.request.urlopen(\"http://data.alexa.com/data?cli=10&dat=s&url=\" + url).read(), \"xml\").find(\"REACH\")['RANK']\n",
    "        except TypeError:\n",
    "            return 1\n",
    "        except HTTPError:\n",
    "            return 2\n",
    "        rank= int(rank)\n",
    "        if (rank<100000):\n",
    "            return 0\n",
    "        else:\n",
    "            return 2\n",
    "        \n",
    "    def domain_registration_length(self,url):\n",
    "        dns = 0\n",
    "        try:\n",
    "            domain_name = whois.whois(urlparse(url).netloc)\n",
    "        except:\n",
    "            dns = 1\n",
    "        \n",
    "        if dns == 1:\n",
    "            return 1      #phishing\n",
    "        else:\n",
    "            expiration_date = domain_name.expiration_date\n",
    "            today = time.strftime('%Y-%m-%d')\n",
    "            today = datetime.strptime(today, '%Y-%m-%d')\n",
    "            if expiration_date is None:\n",
    "                return 1\n",
    "            elif type(expiration_date) is list or type(today) is list :\n",
    "                return 2     #If it is a type of list then we can't select a single value from list. So,it is regarded as suspected website  \n",
    "            else:\n",
    "                creation_date = domain_name.creation_date\n",
    "                expiration_date = domain_name.expiration_date\n",
    "                if (isinstance(creation_date,str) or isinstance(expiration_date,str)):\n",
    "                    try:\n",
    "                        creation_date = datetime.strptime(creation_date,'%Y-%m-%d')\n",
    "                        expiration_date = datetime.strptime(expiration_date,\"%Y-%m-%d\")\n",
    "                    except:\n",
    "                        return 2\n",
    "                registration_length = abs((expiration_date - today).days)\n",
    "                if registration_length / 365 <= 1:\n",
    "                    return 1 #phishing\n",
    "                else:\n",
    "                    return 0 # legitimate\n",
    "            \n",
    "    def age_domain(self,url):\n",
    "        dns = 0\n",
    "        try:\n",
    "            domain_name = whois.whois(urlparse(url).netloc)\n",
    "        except:\n",
    "            dns = 1\n",
    "        \n",
    "        if dns == 1:\n",
    "            return 1\n",
    "        else:\n",
    "            creation_date = domain_name.creation_date\n",
    "            expiration_date = domain_name.expiration_date\n",
    "            if (isinstance(creation_date,str) or isinstance(expiration_date,str)):\n",
    "                try:\n",
    "                    creation_date = datetime.strptime(creation_date,'%Y-%m-%d')\n",
    "                    expiration_date = datetime.strptime(expiration_date,\"%Y-%m-%d\")\n",
    "                except:\n",
    "                    return 2\n",
    "            if ((expiration_date is None) or (creation_date is None)):\n",
    "                return 1\n",
    "            elif ((type(expiration_date) is list) or (type(creation_date) is list)):\n",
    "                return 2\n",
    "            else:\n",
    "                ageofdomain = abs((expiration_date - creation_date).days)\n",
    "                if ((ageofdomain/30) < 6):\n",
    "                    return 1\n",
    "                else:\n",
    "                    return 0\n",
    "     \n",
    "    \n",
    "    def dns_record(self,url):\n",
    "        dns = 0\n",
    "        try:\n",
    "            domain_name = whois.whois(urlparse(url).netloc)\n",
    "            #rint(domain_name)\n",
    "        except:\n",
    "            dns = 1\n",
    "        \n",
    "        if dns == 1:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "   \n",
    "    def statistical_report(self,url):\n",
    "        hostname = url\n",
    "        h = [(x.start(0), x.end(0)) for x in re.finditer('https://|http://|www.|https://www.|http://www.', hostname)]\n",
    "        z = int(len(h))\n",
    "        if z != 0:\n",
    "            y = h[0][1]\n",
    "            hostname = hostname[y:]\n",
    "            h = [(x.start(0), x.end(0)) for x in re.finditer('/', hostname)]\n",
    "            z = int(len(h))\n",
    "            if z != 0:\n",
    "                hostname = hostname[:h[0][0]]\n",
    "        url_match=re.search('at\\.ua|usa\\.cc|baltazarpresentes\\.com\\.br|pe\\.hu|esy\\.es|hol\\.es|sweddy\\.com|myjino\\.ru|96\\.lt|ow\\.ly',url)\n",
    "        try:\n",
    "            ip_address = socket.gethostbyname(hostname)\n",
    "            ip_match=re.search('146\\.112\\.61\\.108|213\\.174\\.157\\.151|121\\.50\\.168\\.88|192\\.185\\.217\\.116|78\\.46\\.211\\.158|181\\.174\\.165\\.13|46\\.242\\.145\\.103|121\\.50\\.168\\.40|83\\.125\\.22\\.219|46\\.242\\.145\\.98|107\\.151\\.148\\.44|107\\.151\\.148\\.107|64\\.70\\.19\\.203|199\\.184\\.144\\.27|107\\.151\\.148\\.108|107\\.151\\.148\\.109|119\\.28\\.52\\.61|54\\.83\\.43\\.69|52\\.69\\.166\\.231|216\\.58\\.192\\.225|118\\.184\\.25\\.86|67\\.208\\.74\\.71|23\\.253\\.126\\.58|104\\.239\\.157\\.210|175\\.126\\.123\\.219|141\\.8\\.224\\.221|10\\.10\\.10\\.10|43\\.229\\.108\\.32|103\\.232\\.215\\.140|69\\.172\\.201\\.153|216\\.218\\.185\\.162|54\\.225\\.104\\.146|103\\.243\\.24\\.98|199\\.59\\.243\\.120|31\\.170\\.160\\.61|213\\.19\\.128\\.77|62\\.113\\.226\\.131|208\\.100\\.26\\.234|195\\.16\\.127\\.102|195\\.16\\.127\\.157|34\\.196\\.13\\.28|103\\.224\\.212\\.222|172\\.217\\.4\\.225|54\\.72\\.9\\.51|192\\.64\\.147\\.141|198\\.200\\.56\\.183|23\\.253\\.164\\.103|52\\.48\\.191\\.26|52\\.214\\.197\\.72|87\\.98\\.255\\.18|209\\.99\\.17\\.27|216\\.38\\.62\\.18|104\\.130\\.124\\.96|47\\.89\\.58\\.141|78\\.46\\.211\\.158|54\\.86\\.225\\.156|54\\.82\\.156\\.19|37\\.157\\.192\\.102|204\\.11\\.56\\.48|110\\.34\\.231\\.42',ip_address)  \n",
    "        except:\n",
    "            return 1\n",
    "\n",
    "        if url_match:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "    def https_token(self,url):\n",
    "        match=re.search('https://|http://',url)\n",
    "        try:\n",
    "            if match.start(0)==0 and match.start(0) is not None:\n",
    "                url=url[match.end(0):]\n",
    "                match=re.search('http|https',url)\n",
    "                if match:\n",
    "                    return 1\n",
    "                else:\n",
    "                    return 0\n",
    "        except:\n",
    "            return 1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL_features=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.fanworkrecs.com/\n"
     ]
    }
   ],
   "source": [
    "# object creation\n",
    "fe = FeatureExtraction()\n",
    "####################################################################\n",
    "url=new_url\n",
    "print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "www.fanworkrecs.com\n"
     ]
    }
   ],
   "source": [
    "URL_features.append(fe.getDomain(url))\n",
    "print(fe.getDomain(url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http\n",
      "/\n"
     ]
    }
   ],
   "source": [
    "URL_features.append(fe.getProtocol(url))\n",
    "print(fe.getProtocol(url))\n",
    "####################################################################_PATH\n",
    "URL_features.append(fe.getPath(url))\n",
    "print(fe.getPath(url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "URL_features.append(fe.havingIP(url))\n",
    "print(fe.havingIP(url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "URL_features.append(fe.long_url(url))\n",
    "print(fe.long_url(url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "URL_features.append(fe.have_at_symbol(url))\n",
    "print(fe.have_at_symbol(url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "URL_features.append(fe.redirection(url))\n",
    "print(fe.redirection(url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "URL_features.append(fe.prefix_suffix_separation(url))\n",
    "print(fe.prefix_suffix_separation(url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "URL_features.append(fe.prefix_suffix_separation(url))\n",
    "print(fe.prefix_suffix_separation(url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "URL_features.append(fe.sub_domains(url))\n",
    "print(fe.sub_domains(url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "URL_features.append(fe.shortening_service(url))\n",
    "print(fe.shortening_service(url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#URL_features.append(fe.web_traffic(url))\n",
    "#print(fe.web_traffic(url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "URL_features.append(fe.domain_registration_length(url))\n",
    "print(fe.domain_registration_length(url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "URL_features.append(fe.dns_record(url))\n",
    "print(fe.dns_record(url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "URL_features.append(fe.statistical_report(url))\n",
    "print(fe.statistical_report(url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "URL_features.append(fe.age_domain(url))\n",
    "print(fe.age_domain(url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "URL_features.append(fe.https_token(url))\n",
    "print(fe.https_token(url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def google_index(url):\\n        user_agent = \\'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/48.0.2564.116 Safari/537.36\\'\\n        headers = { \\'User-Agent\\' : user_agent}\\n        query = {\\'q\\': \\'info:\\' + url}\\n        google = \"https://www.google.com/search?\" + urlencode(query)\\n        #data = requests.get(google, headers=headers,proxies=proxies)\\n        data = requests.get(google,headers=headers)\\n        data.encoding = \\'ISO-8859-1\\'\\n        soup = BeautifulSoup(str(data.content), \"html.parser\")\\n        try:\\n            check = soup.find(id=\"rso\").find(\"div\").find(\"div\").find(\"h3\").find(\"a\")\\n            if soup.find(id=\"rso\").find(\"div\").find(\"div\").find(\"h3\").find(\"a\").find(\"href\" != None):\\n                href = check[\\'href\\']\\n                return 0 # indexed\\n            else:\\n                return 1\\n        except AttributeError:\\n            return 1 # indexed\\n        #print(\"Waiting \" + str(seconds) + \" seconds until checking next URL.\\n\")\\n        #time.sleep(float(seconds))'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''def google_index(url):\n",
    "        user_agent = 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/48.0.2564.116 Safari/537.36'\n",
    "        headers = { 'User-Agent' : user_agent}\n",
    "        query = {'q': 'info:' + url}\n",
    "        google = \"https://www.google.com/search?\" + urlencode(query)\n",
    "        #data = requests.get(google, headers=headers,proxies=proxies)\n",
    "        data = requests.get(google,headers=headers)\n",
    "        data.encoding = 'ISO-8859-1'\n",
    "        soup = BeautifulSoup(str(data.content), \"html.parser\")\n",
    "        try:\n",
    "            check = soup.find(id=\"rso\").find(\"div\").find(\"div\").find(\"h3\").find(\"a\")\n",
    "            if soup.find(id=\"rso\").find(\"div\").find(\"div\").find(\"h3\").find(\"a\").find(\"href\" != None):\n",
    "                href = check['href']\n",
    "                return 0 # indexed\n",
    "            else:\n",
    "                return 1\n",
    "        except AttributeError:\n",
    "            return 1 # indexed\n",
    "        #print(\"Waiting \" + str(seconds) + \" seconds until checking next URL.\\n\")\n",
    "        #time.sleep(float(seconds))'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#URL_features.append(google_index(new_url))\n",
    "#print(google_index(new_url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def abnormal_url(new_url):\n",
    "        dns = 0\n",
    "        #domain_name = \"\"\n",
    "        try:\n",
    "            #domain = urlparse(url).netloc\n",
    "            #print(domain)\n",
    "            domain_name = whois.whois(urlparse(url).netloc)\n",
    "            #print(domain_name)\n",
    "        except:\n",
    "            dns = 1\n",
    "        \n",
    "        if dns == 1:\n",
    "            return 1 # phishing\n",
    "        else:\n",
    "            hostname=domain_name.domain_name\n",
    "            #match=re.search(hostname,url)\n",
    "            if hostname in url:\n",
    "                return 0 # legitimate\n",
    "            else:\n",
    "                return 1 # phishing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#URL_features.append(abnormal_url(url))\n",
    "#print(abnormal_url(url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The features is  ['www.fanworkrecs.com', 'http', '/', 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(\"The features is \",URL_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sensetive  Feature extraction\n",
    "### if u have more than one url  then upload the file \n",
    "#### I have take 100 legitimate url from legitimate list (white list) for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>urls</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>websites</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://www.emuck.com:3000/archive/egan.html</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://danoday.com/summit.shtml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://groups.yahoo.com/group/voice_actor_appr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://voice-international.com/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>http://www.post-gazette.com/magazine/19990223v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>http://www.serkworks.com/roommates/index.htm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>http://www.armory.com/~keeper/jesshirt.html</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>http://www.voicechasers.com/database/showactor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>http://docharris.com/</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>102 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  urls\n",
       "0                                             websites\n",
       "1          http://www.emuck.com:3000/archive/egan.html\n",
       "2                      http://danoday.com/summit.shtml\n",
       "3    http://groups.yahoo.com/group/voice_actor_appr...\n",
       "4                      http://voice-international.com/\n",
       "..                                                 ...\n",
       "97   http://www.post-gazette.com/magazine/19990223v...\n",
       "98        http://www.serkworks.com/roommates/index.htm\n",
       "99         http://www.armory.com/~keeper/jesshirt.html\n",
       "100  http://www.voicechasers.com/database/showactor...\n",
       "101                              http://docharris.com/\n",
       "\n",
       "[102 rows x 1 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "computer_data=pd.read_csv(\"100-legitimate.txt\",header=None,names=['urls'])\n",
    "computer_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "102"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows = len(computer_data)\n",
    "rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://www.emuck.com:3000/archive/egan.html'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "computer_data[\"urls\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#features\n",
    "protocol = []\n",
    "domain = []\n",
    "path = []\n",
    "having_ip = []\n",
    "len_url = []\n",
    "having_at_symbol = []\n",
    "redirection_symbol = []\n",
    "prefix_suffix_separation = []\n",
    "sub_domains = []\n",
    "tiny_url = []\n",
    "abnormal_url = []\n",
    "web_traffic = []\n",
    "domain_registration_length = []\n",
    "dns_record = []\n",
    "statistical_report = []\n",
    "age_domain = []\n",
    "http_tokens = []\n",
    "google_index = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 - legitimate\n",
    "### -1 - phishing\n",
    "### 0 - suspicious"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "websites\n",
      "1\n",
      "http://www.emuck.com:3000/archive/egan.html\n",
      "2\n",
      "http://danoday.com/summit.shtml\n",
      "3\n",
      "http://groups.yahoo.com/group/voice_actor_appreciation/links/events_and_appearanc_000985383761/\n",
      "4\n",
      "http://voice-international.com/\n",
      "5\n",
      "http://www.livinglegendsltd.com/\n",
      "6\n",
      "http://voicechasers.com/forum/viewforum.php?f=8\n",
      "7\n",
      "http://hollywoodcollectorshow.com/\n",
      "8\n",
      "http://www.geocities.com/hollywood/hills/8944/\n",
      "9\n",
      "http://asifa.proboards61.com/index.cgi?action=calendarviewall\n",
      "10\n",
      "http://groups.yahoo.com/group/voice_actor_appreciation/cal///group/voice_actor_appreciation/?v=42&amp;pos=\n",
      "11\n",
      "http://us.imdb.com/name/nm0267724/\n",
      "12\n",
      "http://www.pamelynferdin.com/\n",
      "13\n",
      "http://www.pamferdin.com/\n",
      "14\n",
      "http://www.monkeydog.com/\n",
      "15\n",
      "http://teamknightrider.com/cast/plato/plato.html\n",
      "16\n",
      "http://us.imdb.com/name/nm0281486/\n",
      "17\n",
      "http://anp.awn.com/anifest-3.html\n",
      "18\n",
      "http://www.awn.com/mag/issue1.7/articles/kowlaskidi1.7.html\n",
      "19\n",
      "http://www.awn.com/mag/issue2.7/2.7pages/2.7jacksonjones.html\n",
      "20\n",
      "http://www.awn.com/mag/issue1.9/articles/kowlagrinch1.9.html\n",
      "21\n",
      "http://us.imdb.com/name/nm0004931/\n",
      "22\n",
      "http://www.voicechasers.com/database/showactor.php?actorid=1251\n",
      "23\n",
      "http://www.vortex.com/av.html#jayward1\n",
      "24\n",
      "http://starbulletin.com/96/07/11/features/story3.html\n",
      "25\n",
      "http://anp.awn.com/juneforay.html\n",
      "26\n",
      "http://www.cerias.purdue.edu/homes/spaf/yucks/v2/msg00061.html\n",
      "27\n",
      "http://www.awn.com/mag/issue5.03/5.03pages/evanierforay.php3\n",
      "28\n",
      "http://www.awn.com/mag/issue4.02/4.02pages/foraylittlejohn.php3\n",
      "29\n",
      "http://archives.seattletimes.nwsource.com/cgi-bin/texis.cgi/web/vortex/display?slug=voic29&amp;date=20000629\n",
      "Error trying to connect to socket: closing socket\n",
      "Error trying to connect to socket: closing socket\n",
      "30\n",
      "http://www.geocities.com/enchantedforest/dell/8545/fraley.html\n",
      "31\n",
      "http://us.imdb.com/name/nm0289710/\n",
      "32\n",
      "http://www.voicechasers.com/database/showactor.php?actorid=1133\n",
      "33\n",
      "http://patfraley.com/\n",
      "34\n",
      "http://nerf-herders-anonymous.net/patfraley.html\n",
      "35\n",
      "http://www.vasta.org/professional_index/fraleyp.html\n",
      "36\n",
      "http://news.toonzone.net/2000/oct/27/hearing_voices.php#pat_on_the_head\n",
      "37\n",
      "http://members.tripod.com/~leemichaelwithers/sfh.htm\n",
      "38\n",
      "http://www.radiospirits.com/onradio/meethost_sf.asp\n",
      "39\n",
      "http://us.imdb.com/name/nm0292677/\n",
      "40\n",
      "http://freberg.westnet.com/\n",
      "41\n",
      "http://www.aarp.org/mmaturity/mar_apr00_preview/creators.html#freberg\n",
      "42\n",
      "http://www.cosmik.com/aa-october99/stan_freberg.html\n",
      "43\n",
      "http://www.tsimon.com/freberg.htm\n",
      "44\n",
      "http://www.firezine.net/issue6/fz6_07.htm\n",
      "45\n",
      "http://www.thehutch.com/rabhutch/freberg.htm\n",
      "46\n",
      "http://www.povonline.com/cols/col051.htm\n",
      "Error trying to connect to socket: closing socket\n",
      "47\n",
      "http://www.povonline.com/2001/news060901.htm\n",
      "Error trying to connect to socket: closing socket\n",
      "Error trying to connect to socket: closing socket\n",
      "Error trying to connect to socket: closing socket\n",
      "48\n",
      "http://www.austinchronicle.com/issues/dispatch/1999-12-10/music_feature3.html\n",
      "49\n",
      "http://www.dawsbutler.com/freberg.htm\n",
      "50\n",
      "http://freberg.westnet.com/list.html\n",
      "51\n",
      "http://www.rhino.com/features/75645p.html\n",
      "52\n",
      "http://hitcomedy.com/index.html?/act/freberg/\n",
      "53\n",
      "http://www.cyberonic.net/~atrain/comedy/freberg.htm\n",
      "54\n",
      "http://www.povonline.com/cols/col102.htm\n",
      "55\n",
      "http://freberg.westnet.com/hear_it.html\n",
      "56\n",
      "http://www.epinions.com/musc-album-musc-freberg__stan/tk_~mm032.1.2\n",
      "57\n",
      "http://citypaper.net/articles/091699/mus.stan.shtml\n",
      "58\n",
      "http://www.crispinfreeman.com/\n",
      "59\n",
      "http://www.reelradio.com/gifts/frees.html\n",
      "60\n",
      "http://www.spookytoms.com/paul_frees_tribute.html\n",
      "61\n",
      "http://www.hollowhill.com/gm/library/frees.htm\n",
      "62\n",
      "http://www.dvdtalk.com/dvdsavant/s74frees.html\n",
      "63\n",
      "http://us.imdb.com/name/nm0293659/\n",
      "64\n",
      "http://www.voicechasers.com/database/showactor.php?actorid=1248\n",
      "65\n",
      "http://bakerstreetdozen.com/frewernews.html\n",
      "66\n",
      "http://us.imdb.com/name/nm0001242/\n",
      "67\n",
      "http://www.geocities.com/enchantedforest/dell/8545/va.html\n",
      "68\n",
      "http://www.angelfire.com/ut/rtoons/actors.html\n",
      "69\n",
      "http://www.geocities.com/hollywood/lot/2636/voiceacting/\n",
      "70\n",
      "http://www.geocities.com/philbass_2000/narrators.html\n",
      "71\n",
      "http://doreen.mkbmemorial.com/\n",
      "72\n",
      "http://www.geocities.com/televisioncity/taping/6785/index.html\n",
      "73\n",
      "http://www.behindthevoiceactors.com\n",
      "74\n",
      "http://www.keyframeonline.com/kf.php?op=voic\n",
      "75\n",
      "http://www.voicechasers.com/\n",
      "76\n",
      "http://us.imdb.com/name/nm0304000/\n",
      "77\n",
      "http://www.taoslandandfilm.com/bradentnwkly.html\n",
      "78\n",
      "http://us.imdb.com/name/nm0004951/\n",
      "79\n",
      "http://kathygarver.com/\n",
      "80\n",
      "http://us.imdb.com/name/nm0308744/\n",
      "81\n",
      "http://www.dickgautier.com/\n",
      "82\n",
      "http://www.dangilvezan.com/\n",
      "83\n",
      "http://us.imdb.com/name/nm0319737/\n",
      "84\n",
      "http://us.imdb.com/name/nm0330015/\n",
      "85\n",
      "http://www.tv-now.com/stars/gotfried.html\n",
      "86\n",
      "http://www.cdaccess.com/html/pc/midmovie.htm\n",
      "Error trying to connect to socket: closing socket\n",
      "Error trying to connect to socket: closing socket\n",
      "87\n",
      "http://genetic_mishap.tripod.com/landofthreenamedpeople/\n",
      "88\n",
      "http://www.angelfire.com/celeb/starzbios/gottfried_g.html\n",
      "89\n",
      "http://us.imdb.com/name/nm0331906/\n",
      "90\n",
      "http://www-tech.mit.edu/v110/n37/gilber.37a.html\n",
      "91\n",
      "http://www-tech.mit.edu/v110/n38/lsc.38a.html\n",
      "92\n",
      "http://www.octopusmediaink.com/gilbertgottfried.html\n",
      "93\n",
      "http://citypaper.net/articles/022996/article001.shtml\n",
      "94\n",
      "http://jam.canoe.ca/movies/artists/g/gottfried_gilbert/1997/02/21/758674.html\n",
      "Error trying to connect to socket: closing socket\n",
      "95\n",
      "http://www.voicechasers.com/database/showactor.php?actorid=2148\n",
      "96\n",
      "http://www.kylehebert.com/\n",
      "97\n",
      "http://www.post-gazette.com/magazine/19990223voicetalent1.asp\n",
      "98\n",
      "http://www.serkworks.com/roommates/index.htm\n",
      "99\n",
      "http://www.armory.com/~keeper/jesshirt.html\n",
      "100\n",
      "http://www.voicechasers.com/database/showactor.php?actorid=1220\n",
      "101\n",
      "http://docharris.com/\n",
      "[0, 0, 0, 1, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 0, 2, 0, 0, 0, 2, 2, 2, 1, 2, 0, 2, 0, 0, 0, 2, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 2, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 1, 2, 0, 2, 0, 0, 2, 0]\n",
      "['', 'www.emuck.com:3000', 'danoday.com', 'groups.yahoo.com', 'voice-international.com', 'www.livinglegendsltd.com', 'voicechasers.com', 'hollywoodcollectorshow.com', 'www.geocities.com', 'asifa.proboards61.com', 'groups.yahoo.com', 'us.imdb.com', 'www.pamelynferdin.com', 'www.pamferdin.com', 'www.monkeydog.com', 'teamknightrider.com', 'us.imdb.com', 'anp.awn.com', 'www.awn.com', 'www.awn.com', 'www.awn.com', 'us.imdb.com', 'www.voicechasers.com', 'www.vortex.com', 'starbulletin.com', 'anp.awn.com', 'www.cerias.purdue.edu', 'www.awn.com', 'www.awn.com', 'archives.seattletimes.nwsource.com', 'www.geocities.com', 'us.imdb.com', 'www.voicechasers.com', 'patfraley.com', 'nerf-herders-anonymous.net', 'www.vasta.org', 'news.toonzone.net', 'members.tripod.com', 'www.radiospirits.com', 'us.imdb.com', 'freberg.westnet.com', 'www.aarp.org', 'www.cosmik.com', 'www.tsimon.com', 'www.firezine.net', 'www.thehutch.com', 'www.povonline.com', 'www.povonline.com', 'www.austinchronicle.com', 'www.dawsbutler.com', 'freberg.westnet.com', 'www.rhino.com', 'hitcomedy.com', 'www.cyberonic.net', 'www.povonline.com', 'freberg.westnet.com', 'www.epinions.com', 'citypaper.net', 'www.crispinfreeman.com', 'www.reelradio.com', 'www.spookytoms.com', 'www.hollowhill.com', 'www.dvdtalk.com', 'us.imdb.com', 'www.voicechasers.com', 'bakerstreetdozen.com', 'us.imdb.com', 'www.geocities.com', 'www.angelfire.com', 'www.geocities.com', 'www.geocities.com', 'doreen.mkbmemorial.com', 'www.geocities.com', 'www.behindthevoiceactors.com', 'www.keyframeonline.com', 'www.voicechasers.com', 'us.imdb.com', 'www.taoslandandfilm.com', 'us.imdb.com', 'kathygarver.com', 'us.imdb.com', 'www.dickgautier.com', 'www.dangilvezan.com', 'us.imdb.com', 'us.imdb.com', 'www.tv-now.com', 'www.cdaccess.com', 'genetic_mishap.tripod.com', 'www.angelfire.com', 'us.imdb.com', 'www-tech.mit.edu', 'www-tech.mit.edu', 'www.octopusmediaink.com', 'citypaper.net', 'jam.canoe.ca', 'www.voicechasers.com', 'www.kylehebert.com', 'www.post-gazette.com', 'www.serkworks.com', 'www.armory.com', 'www.voicechasers.com', 'docharris.com']\n",
      "['websites', '/archive/egan.html', '/summit.shtml', '/group/voice_actor_appreciation/links/events_and_appearanc_000985383761/', '/', '/', '/forum/viewforum.php', '/', '/hollywood/hills/8944/', '/index.cgi', '/group/voice_actor_appreciation/cal///group/voice_actor_appreciation/', '/name/nm0267724/', '/', '/', '/', '/cast/plato/plato.html', '/name/nm0281486/', '/anifest-3.html', '/mag/issue1.7/articles/kowlaskidi1.7.html', '/mag/issue2.7/2.7pages/2.7jacksonjones.html', '/mag/issue1.9/articles/kowlagrinch1.9.html', '/name/nm0004931/', '/database/showactor.php', '/av.html', '/96/07/11/features/story3.html', '/juneforay.html', '/homes/spaf/yucks/v2/msg00061.html', '/mag/issue5.03/5.03pages/evanierforay.php3', '/mag/issue4.02/4.02pages/foraylittlejohn.php3', '/cgi-bin/texis.cgi/web/vortex/display', '/enchantedforest/dell/8545/fraley.html', '/name/nm0289710/', '/database/showactor.php', '/', '/patfraley.html', '/professional_index/fraleyp.html', '/2000/oct/27/hearing_voices.php', '/~leemichaelwithers/sfh.htm', '/onradio/meethost_sf.asp', '/name/nm0292677/', '/', '/mmaturity/mar_apr00_preview/creators.html', '/aa-october99/stan_freberg.html', '/freberg.htm', '/issue6/fz6_07.htm', '/rabhutch/freberg.htm', '/cols/col051.htm', '/2001/news060901.htm', '/issues/dispatch/1999-12-10/music_feature3.html', '/freberg.htm', '/list.html', '/features/75645p.html', '/index.html', '/~atrain/comedy/freberg.htm', '/cols/col102.htm', '/hear_it.html', '/musc-album-musc-freberg__stan/tk_~mm032.1.2', '/articles/091699/mus.stan.shtml', '/', '/gifts/frees.html', '/paul_frees_tribute.html', '/gm/library/frees.htm', '/dvdsavant/s74frees.html', '/name/nm0293659/', '/database/showactor.php', '/frewernews.html', '/name/nm0001242/', '/enchantedforest/dell/8545/va.html', '/ut/rtoons/actors.html', '/hollywood/lot/2636/voiceacting/', '/philbass_2000/narrators.html', '/', '/televisioncity/taping/6785/index.html', '', '/kf.php', '/', '/name/nm0304000/', '/bradentnwkly.html', '/name/nm0004951/', '/', '/name/nm0308744/', '/', '/', '/name/nm0319737/', '/name/nm0330015/', '/stars/gotfried.html', '/html/pc/midmovie.htm', '/landofthreenamedpeople/', '/celeb/starzbios/gottfried_g.html', '/name/nm0331906/', '/v110/n37/gilber.37a.html', '/v110/n38/lsc.38a.html', '/gilbertgottfried.html', '/articles/022996/article001.shtml', '/movies/artists/g/gottfried_gilbert/1997/02/21/758674.html', '/database/showactor.php', '/', '/magazine/19990223voicetalent1.asp', '/roommates/index.htm', '/~keeper/jesshirt.html', '/database/showactor.php', '/']\n",
      "['', 'http', 'http', 'http', 'http', 'http', 'http', 'http', 'http', 'http', 'http', 'http', 'http', 'http', 'http', 'http', 'http', 'http', 'http', 'http', 'http', 'http', 'http', 'http', 'http', 'http', 'http', 'http', 'http', 'http', 'http', 'http', 'http', 'http', 'http', 'http', 'http', 'http', 'http', 'http', 'http', 'http', 'http', 'http', 'http', 'http', 'http', 'http', 'http', 'http', 'http', 'http', 'http', 'http', 'http', 'http', 'http', 'http', 'http', 'http', 'http', 'http', 'http', 'http', 'http', 'http', 'http', 'http', 'http', 'http', 'http', 'http', 'http', 'http', 'http', 'http', 'http', 'http', 'http', 'http', 'http', 'http', 'http', 'http', 'http', 'http', 'http', 'http', 'http', 'http', 'http', 'http', 'http', 'http', 'http', 'http', 'http', 'http', 'http', 'http', 'http', 'http']\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
      "[0, 2, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 0, 2, 2, 0, 2, 1, 1, 1, 1, 2, 0, 2, 0, 0, 2, 2, 2, 2, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 1, 2, 0, 2, 2, 2, 2, 0, 2, 0, 0, 2, 2, 0, 2, 0, 2, 0, 2, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 2, 2, 0, 2, 0, 1, 1, 2, 0, 2, 2, 0, 2, 2, 2, 2, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
      "[1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 2, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 0, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 0, 1, 1, 2, 0, 2, 0, 2, 1, 0, 1, 1, 2, 1, 0, 0, 1, 1, 1, 0, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 2, 1, 2, 2, 1, 2, 0, 1, 1, 2, 0, 2, 1, 2, 2, 1, 2, 2, 2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 1, 2, 2, 0, 0, 0, 0, 2, 0, 2, 2, 0, 0, 0, 0, 2, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 2, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 2, 0, 2, 2, 0, 2, 0, 0, 0, 2, 0, 2, 0, 2, 2, 0, 2, 2, 2, 0, 0, 0, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# object creation\n",
    "fe = FeatureExtraction()\n",
    "#rows = len(computer_data[\"urls\"])\n",
    "\n",
    "for i in range(0,rows):\n",
    "    url=computer_data[\"urls\"][i]\n",
    "####################################################################_THE URL\n",
    "    print(i )\n",
    "    print(url)\n",
    "#####################################################################_PROTOCOL\n",
    "    protocol.append(fe.getProtocol(url)) \n",
    "####################################################################_PATH\n",
    "    path.append(fe.getPath(url))\n",
    "###################################################################_DOMAIN\n",
    "    domain.append(fe.getDomain(url))\n",
    "###################################################################_IP ADDRESS\n",
    "    having_ip.append(fe.havingIP(url))\n",
    "###################################################################_LENTH OF URL\n",
    "    len_url.append(fe.long_url(url))\n",
    "##################################################################_HAVING @ SYMBOL\n",
    "    having_at_symbol.append(fe.have_at_symbol(url))\n",
    "#################################################################_HAVVING // SYMBOL\n",
    "    redirection_symbol.append(fe.redirection(url))\n",
    "###################################################################_HAVING PREFIX & SUFFIX\n",
    "    prefix_suffix_separation.append(fe.prefix_suffix_separation(url))\n",
    "#######################################################################_SUBDOMAINS\n",
    "    sub_domains.append(fe.sub_domains(url))\n",
    "#####################################################################_TINY URL\n",
    "    tiny_url.append(fe.shortening_service(url))\n",
    "##########################################################################_WEB TRAFIC \n",
    "    #web_traffic.append(fe.web_traffic(url))            #NOT WORKING\n",
    "##################################################################################_DOMAIN  REGISTRTER LENTH\n",
    "    domain_registration_length.append(fe.domain_registration_length(url))\n",
    "##################################################################################_DNS RECORD\n",
    "    dns_record.append(fe.dns_record(url))\n",
    "###############################################################_STATISTICAL REPORT\n",
    "    statistical_report.append(fe.statistical_report(url))\n",
    "####################################################################_AGE OF DOMAIN\n",
    "    age_domain.append(fe.age_domain(url))\n",
    "####################################################################_HTTP TOKENS\n",
    "    http_tokens.append(fe.https_token(url))\n",
    "####################################################################_GOOGLE INDEX\n",
    "    #google_index.append(fe.google_index(url))\n",
    "#################################################################_ABNORMAL URL\n",
    "    #abnormal_url.append(fe.abnormal_url(url)) \n",
    "###################################################################\n",
    "print(len_url)\n",
    "print(domain)\n",
    "print(path)\n",
    "print(protocol)\n",
    "print(having_ip)\n",
    "print(having_at_symbol)\n",
    "print(redirection_symbol)\n",
    "print(prefix_suffix_separation)\n",
    "print(sub_domains)\n",
    "print(tiny_url)\n",
    "#print(web_traffic)\n",
    "print(domain_registration_length)\n",
    "print(dns_record)\n",
    "print(statistical_report)\n",
    "print(http_tokens)\n",
    "print(age_domain)\n",
    "#print(google_index)\n",
    "#print(abnormal_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = []\n",
    "for i in range(0,rows):\n",
    "    label.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\soume\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Protocol</th>\n",
       "      <th>Domain</th>\n",
       "      <th>Path</th>\n",
       "      <th>Having_IP</th>\n",
       "      <th>URL_Length</th>\n",
       "      <th>Having_@_symbol</th>\n",
       "      <th>Redirection_//_symbol</th>\n",
       "      <th>Prefix_suffix_separation</th>\n",
       "      <th>Sub_domains</th>\n",
       "      <th>tiny_url</th>\n",
       "      <th>web_traffic</th>\n",
       "      <th>domain_registration_length</th>\n",
       "      <th>dns_record</th>\n",
       "      <th>statistical_report</th>\n",
       "      <th>age_domain</th>\n",
       "      <th>http_tokens</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>websites</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http</td>\n",
       "      <td>www.emuck.com:3000</td>\n",
       "      <td>/archive/egan.html</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http</td>\n",
       "      <td>danoday.com</td>\n",
       "      <td>/summit.shtml</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http</td>\n",
       "      <td>groups.yahoo.com</td>\n",
       "      <td>/group/voice_actor_appreciation/links/events_a...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http</td>\n",
       "      <td>voice-international.com</td>\n",
       "      <td>/</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>http</td>\n",
       "      <td>www.post-gazette.com</td>\n",
       "      <td>/magazine/19990223voicetalent1.asp</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>http</td>\n",
       "      <td>www.serkworks.com</td>\n",
       "      <td>/roommates/index.htm</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>http</td>\n",
       "      <td>www.armory.com</td>\n",
       "      <td>/~keeper/jesshirt.html</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>http</td>\n",
       "      <td>www.voicechasers.com</td>\n",
       "      <td>/database/showactor.php</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>http</td>\n",
       "      <td>docharris.com</td>\n",
       "      <td>/</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>102 rows Ã— 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Protocol                   Domain  \\\n",
       "0                                       \n",
       "1       http       www.emuck.com:3000   \n",
       "2       http              danoday.com   \n",
       "3       http         groups.yahoo.com   \n",
       "4       http  voice-international.com   \n",
       "..       ...                      ...   \n",
       "97      http     www.post-gazette.com   \n",
       "98      http        www.serkworks.com   \n",
       "99      http           www.armory.com   \n",
       "100     http     www.voicechasers.com   \n",
       "101     http            docharris.com   \n",
       "\n",
       "                                                  Path  Having_IP  URL_Length  \\\n",
       "0                                             websites          0           0   \n",
       "1                                   /archive/egan.html          0           0   \n",
       "2                                        /summit.shtml          0           0   \n",
       "3    /group/voice_actor_appreciation/links/events_a...          0           1   \n",
       "4                                                    /          0           0   \n",
       "..                                                 ...        ...         ...   \n",
       "97                  /magazine/19990223voicetalent1.asp          0           2   \n",
       "98                                /roommates/index.htm          0           0   \n",
       "99                              /~keeper/jesshirt.html          0           0   \n",
       "100                            /database/showactor.php          0           2   \n",
       "101                                                  /          0           0   \n",
       "\n",
       "     Having_@_symbol  Redirection_//_symbol  Prefix_suffix_separation  \\\n",
       "0                  0                      0                         0   \n",
       "1                  0                      0                         0   \n",
       "2                  0                      0                         0   \n",
       "3                  0                      0                         0   \n",
       "4                  0                      0                         1   \n",
       "..               ...                    ...                       ...   \n",
       "97                 0                      0                         1   \n",
       "98                 0                      0                         0   \n",
       "99                 0                      0                         0   \n",
       "100                0                      0                         0   \n",
       "101                0                      0                         0   \n",
       "\n",
       "     Sub_domains  tiny_url  web_traffic  domain_registration_length  \\\n",
       "0              0         0          NaN                           1   \n",
       "1              2         0          NaN                           1   \n",
       "2              0         0          NaN                           1   \n",
       "3              0         0          NaN                           2   \n",
       "4              0         0          NaN                           1   \n",
       "..           ...       ...          ...                         ...   \n",
       "97             2         0          NaN                           0   \n",
       "98             2         0          NaN                           1   \n",
       "99             2         0          NaN                           1   \n",
       "100            2         0          NaN                           1   \n",
       "101            0         0          NaN                           1   \n",
       "\n",
       "     dns_record  statistical_report  age_domain  http_tokens  label  \n",
       "0             0                   1           1            1      1  \n",
       "1             0                   1           1            0      1  \n",
       "2             0                   0           2            0      1  \n",
       "3             0                   0           2            0      1  \n",
       "4             0                   0           0            0      1  \n",
       "..          ...                 ...         ...          ...    ...  \n",
       "97            0                   0           0            0      1  \n",
       "98            0                   0           0            0      1  \n",
       "99            0                   0           0            0      1  \n",
       "100           0                   0           0            0      1  \n",
       "101           0                   0           0            0      1  \n",
       "\n",
       "[102 rows x 17 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d={'Protocol':pd.Series(protocol),'Domain':pd.Series(domain),'Path':pd.Series(path),'Having_IP':pd.Series(having_ip),\n",
    "   'URL_Length':pd.Series(len_url),'Having_@_symbol':pd.Series(having_at_symbol),\n",
    "   'Redirection_//_symbol':pd.Series(redirection_symbol),'Prefix_suffix_separation':pd.Series(prefix_suffix_separation),\n",
    "   'Sub_domains':pd.Series(sub_domains),'tiny_url':pd.Series(tiny_url),'web_traffic' : pd.Series(web_traffic) ,\n",
    "   'domain_registration_length':pd.Series(domain_registration_length),'dns_record':pd.Series(dns_record),\n",
    "   'statistical_report':pd.Series(statistical_report),'age_domain':pd.Series(age_domain),'http_tokens':pd.Series(http_tokens),\n",
    "   'label':pd.Series(label)}\n",
    "data=pd.DataFrame(d)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(\"feature_extraction_trying.csv\",index=False,encoding='UTF-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
